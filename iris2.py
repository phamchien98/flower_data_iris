# -*- coding: utf-8 -*-
"""iris2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_X4Q1wahJ669JhEGiFVmMMcpiadZT-Tm
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import sklearn
import os



os.listdir()
iris = pd.read_csv("./Iris.csv")
iris["Species"] = iris["Species"].map({"Iris-setosa":0,"Iris-virginica":1,"Iris-versicolor":2})

iris.head()

iris_X = iris[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']].values
iris_y = iris[['Species']].values

print(type(iris_X))

print(iris_y)

y_onehot=np.zeros(shape=(iris_y.shape[0],3))
for i in range(iris_y.shape[0]):
  label=iris_y[i]
#   print(label)
  if label==0:
    y_onehot[i,0]=1
  if label==1:
    y_onehot[i,1]=1
  if label==2:
    y_onehot[i,2]=1
#print(type(iris_X))

print(y_onehot[0:4])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(iris_X,y_onehot, test_size=0.2, random_state=1)
X_train.shape
y_train.shape

#=== tao model ===
x_input = tf.placeholder(tf.float32,shape= [None,4],name = 'x_input')
y_input = tf.placeholder(tf.float32,shape= [None,3],name = 'y_input')

weight_1=tf.Variable(tf.random.truncated_normal(shape=(6,4)),name='weight_1')
bias_1=tf.Variable(tf.random.truncated_normal(shape=(1,6)),name='bias_1')

weight_2=tf.Variable(tf.random.truncated_normal(shape=(5,6)),name='weight_2')
bias_2=tf.Variable(tf.random.truncated_normal(shape=(1,5)),name='bias_2')

weight_3=tf.Variable(tf.random.truncated_normal(shape=(3,5)),name='weight_3')
bias_3=tf.Variable(tf.random.truncated_normal(shape=(1,3)),name='bias_3')

out_layer_1=tf.math.add(tf.linalg.matmul(x_input,tf.transpose(weight_1)),bias_1)
activate_layer_1=tf.nn.relu(out_layer_1)
out_layer_2=tf.math.add(tf.linalg.matmul(activate_layer_1,tf.transpose(weight_2)),bias_2)
activate_layer_2=tf.nn.relu(out_layer_2)
logit=tf.math.add(tf.linalg.matmul(activate_layer_2,tf.transpose(weight_3)),bias_3)

predict=tf.math.argmax(logit,axis=1)


loss=tf.nn.softmax_cross_entropy_with_logits(
    labels=y_input,
    logits=logit,
)
loss=tf.math.reduce_sum(loss)
# #===== end model===

# #===== start learning algorithm ====
optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.001)

thaotac_gan_cho_tat_ca_cac_bien=optimizer.minimize(loss=loss)

sess=tf.Session()

sess.run(tf.global_variables_initializer())

print(sess.run([loss],feed_dict={x_input:X_train,y_input:y_train}))

for _ in range(300):
  sess.run([thaotac_gan_cho_tat_ca_cac_bien],feed_dict={x_input:X_train,y_input:y_train})

print(sess.run([loss],feed_dict={x_input:X_train,y_input:y_train}))

predict_result=sess.run([predict],feed_dict={x_input:X_train})[0]

def accuracy(predict,label):
  y_decode=np.argmax(label,axis=1)
  count=0
  for i in range(0,y_decode.shape[0]):
    if predict[i]==y_decode[i]:
      count+=1
  print(count/y_decode.shape[0])
accuracy(predict_result,y_train)

predict_result=sess.run([predict],feed_dict={x_input:X_test})[0]
accuracy(predict_result,y_test)

predict_result

